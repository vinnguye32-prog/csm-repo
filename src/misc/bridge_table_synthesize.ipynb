{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9262a0cb-8456-43f2-895d-273aaca7285a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read and randomize user_id and contract"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.getActiveSession()\n",
    "\n",
    "# Read user_ids from dim_user\n",
    "user_df = spark.table('workspace.csm_project.dim_user').select('user_id').dropna()\n",
    "user_ids = [row['user_id'] for row in user_df.collect()]\n",
    "random.shuffle(user_ids)\n",
    "user_ids = user_ids[:10000]\n",
    "\n",
    "# Read contracts from dim_contract\n",
    "contract_df = spark.table('workspace.csm_project.dim_contract').select('contract').dropna()\n",
    "contracts = [row['contract'] for row in contract_df.collect()]\n",
    "random.shuffle(contracts)\n",
    "contracts = contracts[:2000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09649d74-a41e-4fef-946c-b885ad684b75",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate user-contract bridge table with schema"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize user-contract assignments\n",
    "data = []\n",
    "user_contracts = {uid: set() for uid in user_ids}\n",
    "contract_users = {cid: set() for cid in contracts}\n",
    "\n",
    "# Assign users to contracts with randomized 1-5 users per contract\n",
    "for contract in contracts:\n",
    "    max_users = random.randint(1, 5)\n",
    "    possible_users = [uid for uid in user_ids if len(user_contracts[uid]) < 5]\n",
    "    assigned_users = random.sample(possible_users, min(max_users, len(possible_users)))\n",
    "    for uid in assigned_users:\n",
    "        user_contracts[uid].add(contract)\n",
    "        contract_users[contract].add(uid)\n",
    "        data.append({\"user_id\": uid, \"contract_id\": contract})\n",
    "\n",
    "bridge_df = pd.DataFrame(data)\n",
    "bridge_df = bridge_df.drop_duplicates()\n",
    "\n",
    "# Add start_date and end_date columns\n",
    "bridge_df['start_date'] = pd.to_datetime('2022-04-01')\n",
    "bridge_df['end_date'] = pd.to_datetime('2026-12-12')\n",
    "\n",
    "# Randomize is_primary: only one user per contract is primary\n",
    "bridge_df['is_primary'] = False\n",
    "for contract in bridge_df['contract_id'].unique():\n",
    "    contract_users = bridge_df[bridge_df['contract_id'] == contract].index.tolist()\n",
    "    if contract_users:\n",
    "        primary_idx = random.choice(contract_users)\n",
    "        bridge_df.at[primary_idx, 'is_primary'] = True\n",
    "\n",
    "bridge_df.sort_values(by = 'contract_id').head(10)\n",
    "bridge_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a898445d-051e-4161-a183-c658f1d60426",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Write bridge_df to user_contract_details (overwrite)"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "# Convert pandas datetime columns to string in 'YYYY-MM-DD' format\n",
    "bridge_df['start_date'] = bridge_df['start_date'].dt.strftime('%Y-%m-%d')\n",
    "bridge_df['end_date'] = bridge_df['end_date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Convert pandas DataFrame to Spark DataFrame\n",
    "spark_bridge_df = spark.createDataFrame(bridge_df)\n",
    "\n",
    "# Cast start_date and end_date to Spark DATE type\n",
    "spark_bridge_df = spark_bridge_df.withColumn('start_date', to_date(col('start_date'), 'yyyy-MM-dd'))\n",
    "spark_bridge_df = spark_bridge_df.withColumn('end_date', to_date(col('end_date'), 'yyyy-MM-dd'))\n",
    "\n",
    "# Write to Delta table\n",
    "spark_bridge_df.write.mode('overwrite').saveAsTable('workspace.csm_project.user_contract_details')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f00f73a6-d90b-41ce-86db-49c8dd616985",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bridge_table_synthesize",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
